# Вот пошаговое объяснение функции naive_bayes:
# Функция принимает два аргумента: data (список данных) и X (список  признаков для нового наблюдения,
# для которого требуется определить вероятность ухода). Подсчитываются общее
# количество наблюдений (total), количество случаев ухода (yes_count) и количество случаев не ухода (no_count).
# Вычисляются априорные вероятности ухода и не ухода (P(C_1) и P(C_2)): p_yes и p_no. Инициализируются вероятности P(
# Х|Уход=Да) и P(Х|Уход=Нет) как априорные вероятности ухода и не ухода соответственно: p_yes_given_X и p_no_given_X.
# Проходим через все признаки в наблюдении X и для каждого признака подсчитываем количество случаев ухода и не ухода,
# когда признак принимает заданное значение. Вычисляем условные вероятности для каждого признака, когда уход равен
# "Да" и "Нет": p_feature_given_yes и p_feature_given_no. Обновляем вероятности P(Х|Уход=Да) и P(Х|Уход=Нет),
# умножая их на условные вероятности для каждого признака. Вычисляем сумму вероятностей P(Х|Уход=Да) и P(Х|Уход=Нет):
# p_X. Нормализуем вероятности P(Х|Уход=Да) и P(Х|Уход=Нет), разделив их на сумму вероятностей p_X. Возвращаем
# вероятности P(Уход=Да|Х) и P(Уход=Нет|Х) как результат работы функции. Функция naive_bayes может быть использована
# для определения вероятности ухода для любого наблюдения с заданными признаками на основе предоставленных данных.
# Это делает ее универсальным инструментом для решения подобных задач с использованием наивного Байесовского
# классификатора.


# Для реализации наивного Байеса на Python, сначала запишем данные из таблицы в виде списка:
data = [
    ['+', '-', '+', '+'],
    ['+', '+', '-', '-'],
    ['-', '-', '+', '+'],
    ['-', '-', '-', '+'],
    ['-', '+', '+', '-'],
    ['+', '-', '+', '-'],
    ['+', '+', '-', '-'],
    ['+', '-', '-', '+'],
    ['-', '+', '-', '-'],
    ['-', '-', '+', '-'],
]

# Варианты X
X1 = ['+', '-', '+']
X2 = ['+', '+', '-']
X3 = ['-', '-', '+']
X4 = ['-', '+', '-']


# Создадим функцию для расчета вероятностей:

def naive_bayes(data, X):
    total = len(data)
    yes_count = sum([1 for row in data if row[-1] == '+'])
    no_count = total - yes_count

    # Априорная вероятность
    p_yes = yes_count / total
    p_no = no_count / total

    p_yes_given_X = p_yes
    p_no_given_X = p_no

    for i in range(len(X)):
        yes_count_feature = sum([1 for row in data if row[i] == X[i] and row[-1] == '+'])
        no_count_feature = sum([1 for row in data if row[i] == X[i] and row[-1] == '-'])

        # Условные вероятности для нового столбца рассчитываются с помощью следующих строк кода:
        p_feature_given_yes = yes_count_feature / yes_count
        p_feature_given_no = no_count_feature / no_count

        # Теперь рассчитаем произведения вероятностей для классов С1 и С2
        p_yes_given_X *= p_feature_given_yes
        p_no_given_X *= p_feature_given_no

    # нормализуются
    p_X = p_yes_given_X + p_no_given_X
    p_yes_given_X /= p_X
    p_no_given_X /= p_X

    return p_yes_given_X, p_no_given_X


p_yes_given_X1, p_no_given_X1 = naive_bayes(data, X1)
print(f"Для X1: P(Уход=Да|X1) = {p_yes_given_X1:.3f}, P(Уход=Нет|X1) = {p_no_given_X1:.3f}")

# Теперь вы можете использовать эту функцию для вычисления вероятности ухода постоянного читателя для каждого из
# наблюдений Х1, Х2, Х3 и Х4:

for i, X in enumerate([X1, X2, X3, X4], start=1):
    p_yes_given_X, p_no_given_X = naive_bayes(data, X)
    print(f"Для X{i}: P(Уход=Да|X{i}) = {p_yes_given_X:.3f}, P(Уход=Нет|X{i}) = {p_no_given_X:.3f}")

# Этот код выполнит расчеты, указанные в вашем примере, и выведет вероятности ухода для каждого из наблюдений Х1, Х2,
# Х3 и Х4.
